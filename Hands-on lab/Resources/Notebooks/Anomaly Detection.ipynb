{"cells":[{"cell_type":"markdown","source":["#Anomaly Detection model for Predictive Maintenance"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d294fa8c-4334-497f-820c-b9f1c203de11"}}},{"cell_type":"code","source":["dbutils.library.installPyPI('azureml-sdk')\ndbutils.library.installPyPI('keras')\ndbutils.library.installPyPI('tensorflow', version='2.2.1')\ndbutils.library.installPyPI('statsmodels')\ndbutils.library.installPyPI('joblib')\ndbutils.library.installPyPI('seaborn')\ndbutils.library.installPyPI('h5py', version='2.10.0')\n\n#best practice is to restart python after installing libraries\ndbutils.library.restartPython()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32dcf747-cbf9-4278-bc06-591ff71ed712"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Import the necessary libraries/namespaces."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acdc3f8c-0670-48b0-ad6f-982cf113bea8"}}},{"cell_type":"code","source":["import requests\nimport os\nimport uuid\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\nimport joblib\nimport h5py\n\nfrom numpy.random import seed\nimport tensorflow as tf\n\nfrom keras.layers import Input, Dropout\nfrom keras.layers.core import Dense \nfrom keras.models import Model, Sequential, load_model\nfrom keras import regularizers\nfrom keras.models import model_from_json\n\nimport azureml\nfrom azureml.core import Run\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model as AmlModel\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7621e46-70bc-4fd8-a9f4-8627798f8973"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Read the raw data that will be used to showcase the anomaly detection models. The following data sets are available:\n- Normal: contains sensor readings that correspond to operation under normal conditions\n- Gradual failure: contains sensor readings that correspond to an initial period of normal operation followed by a period of gradual deterioration and then by one of failure\n- Immediate failure: contains sensor readings that correspond to an initial period of normal operation followed by a sudden transition to failure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49eefad2-cad1-47d7-a572-eb9eeae2c8da"}}},{"cell_type":"code","source":["pdNormal = pd.read_csv('https://databricksdemostore.blob.core.windows.net/data/mcw-predictive-maintenance-for-remote-field-devices/TelemetryNormal.csv')\npdGradualFailure = pd.read_csv('https://databricksdemostore.blob.core.windows.net/data/mcw-predictive-maintenance-for-remote-field-devices/TelemetryWithGradualFailures.csv')\npdImmediateFailure = pd.read_csv('https://databricksdemostore.blob.core.windows.net/data/mcw-predictive-maintenance-for-remote-field-devices/TelemetryWithImmediateFailures.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ba14f16-9226-4107-a278-d7637bd64b70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's look at the timeseries for the five sensors when they operate under normal conditions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c539c70-2889-49df-a046-246bc719e7c7"}}},{"cell_type":"code","source":["fig, ax = plt.subplots(3,2, figsize=(30,10))\n\nax[0][0].set_title('MotorPowerkW')\nax[0][1].set_title('MotorSpeed')\nax[1][0].set_title('PumpRate')\nax[1][1].set_title('TimePumpOn')\nax[2][0].set_title('CasingFriction')\n\nax[0][0].plot(pdNormal['MotorPowerkW'])\nax[0][1].plot(pdNormal['MotorSpeed'])\nax[1][0].plot(pdNormal['PumpRate'])\nax[1][1].plot(pdNormal['TimePumpOn'])\nax[2][0].plot(pdNormal['CasingFriction'])\nfig.suptitle('Sensor data for operations under normal conditions', fontsize=16)\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"209864a2-80cf-4235-8edf-c9601db60673"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's look at the timeseries for the five sensors when they operate initially under normal conditions and then the conditions start to gradually deteriorate until they reach a failure state."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"967f8eeb-0aa5-449e-92c1-f560cc6e01ec"}}},{"cell_type":"code","source":["fig, ax = plt.subplots(3,2, figsize=(30,10))\n\nax[0][0].set_title('MotorPowerkW')\nax[0][1].set_title('MotorSpeed')\nax[1][0].set_title('PumpRate')\nax[1][1].set_title('TimePumpOn')\nax[2][0].set_title('CasingFriction')\n\nax[0][0].plot(pdGradualFailure['MotorPowerkW'])\nax[0][1].plot(pdGradualFailure['MotorSpeed'])\nax[1][0].plot(pdGradualFailure['PumpRate'])\nax[1][1].plot(pdGradualFailure['TimePumpOn'])\nax[2][0].plot(pdGradualFailure['CasingFriction'])\nfig.suptitle('Sensor data for operations when gradual failure occurs', fontsize=16)\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc39e677-ae34-4924-b5f5-4b3fc0302d33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's look at the timeseries for the five sensors when they operate initially under normal conditions and then the conditions suddenly deteriorate reaching a state of failure."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2880d8f-0009-4065-a816-d61373fc3339"}}},{"cell_type":"code","source":["fig, ax = plt.subplots(3,2, figsize=(30,10))\n\nax[0][0].set_title('MotorPowerkW')\nax[0][1].set_title('MotorSpeed')\nax[1][0].set_title('PumpRate')\nax[1][1].set_title('TimePumpOn')\nax[2][0].set_title('CasingFriction')\n\nax[0][0].plot(pdImmediateFailure['MotorPowerkW'])\nax[0][1].plot(pdImmediateFailure['MotorSpeed'])\nax[1][0].plot(pdImmediateFailure['PumpRate'])\nax[1][1].plot(pdImmediateFailure['TimePumpOn'])\nax[2][0].plot(pdImmediateFailure['CasingFriction'])\nfig.suptitle('Sensor data for operations when immediate failure occurs', fontsize=16)\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"822b405c-36e5-45d7-8530-c46f620ed2c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We are scaling all three data sets to make sure all values are properly normalized. For this, we are using the MinMax scaler approach."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a0b9d47-89c5-4faa-bf78-d9d55374412a"}}},{"cell_type":"code","source":["scaler = preprocessing.MinMaxScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(pdNormal),\n                      columns=pdNormal.columns,\n                      index=pdNormal.index)\njoblib.dump(scaler, 'scaler.pkl')\nX_test1 = pd.DataFrame(scaler.transform(pdGradualFailure),\n                      columns=pdGradualFailure.columns,\n                      index=pdGradualFailure.index)\nX_test2 = pd.DataFrame(scaler.transform(pdImmediateFailure),\n                      columns=pdImmediateFailure.columns,\n                      index=pdImmediateFailure.index)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b2653ef-3445-477e-9c9a-2f7b7754b0f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The approach we use for anomaly detection is based on a deep learning model that is trained to recognize normal operating conditions. \nWe are using a special type of deep learning model called an autoencoder. It basically trains to produce outputs that are as similar as possible to the inputs it gets (hence the name autoencoder). As you will see when we actually train the model, we use the same dataset (X_train - sensors under normal conditions) as both training and test data. \n\nNotice the middle layer of the neural network - the one we named 'feature_vector'. When data reaches this layer it is in the for of a 20-dimensional vector. The layer is very important because this is where we 'split' the trained model. Splitting the trained model here and using this layer as an output means that we have a model that is capable of encoding a combination of five sensor readings into a 20-dimensional vector. This piece of the model is called an encoder (for obvious reasons) and we will use it later to better understand conditions that classify as anomalies.\n\nAlso notice that we are using MSE (Mean Squared Error) as the loss function of the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b52f43b9-696b-4524-a136-59cf446ef047"}}},{"cell_type":"code","source":["seed(10)\ntf.random.set_seed(10)\nact_func = 'elu'\n\ninput = Input(shape=(X_train.shape[1],))\nx = Dense(100,activation=act_func, kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0))(input)\nx = Dense(50,activation=act_func, kernel_initializer='glorot_uniform')(x)\nencoder = Dense(20,activation=act_func, kernel_initializer='glorot_uniform', name='feature_vector')(x)\nx = Dense(50,activation=act_func, kernel_initializer='glorot_uniform')(encoder)\nx = Dense(100,activation=act_func, kernel_initializer='glorot_uniform')(x)\noutput = Dense(X_train.shape[1],activation=act_func, kernel_initializer='glorot_uniform')(x)\n\nmodel = Model(input, output)\nmodel.compile(loss='mse',optimizer='adam')\n\nencoder_model = Model(inputs=model.input, outputs=model.get_layer('feature_vector').output)\nencoder_model.compile(loss='mse',optimizer='adam')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4892c5df-4c33-4bfc-acee-dafd40b87b70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's train the autoencoder model using the X_train dataset (sensors operating under normal conditions) as both training and test data.\n\nFor model validation, we are using a 95% / 5% split."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a8efe8f-0485-44bd-9666-4dcac33b5d4d"}}},{"cell_type":"code","source":["epochs = 100\nbatch_size = 10\n\nhistory=model.fit(np.array(X_train),np.array(X_train),\n                  batch_size=batch_size, \n                  epochs=epochs,\n                  validation_split=0.05,\n                  verbose = 1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d836a07e-c8a0-4edc-bd25-c48229952679"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Next, let's take a look at how the loss function (MSE) evolved for both training and validation data during the advancing of the epochs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce9644e-a2a7-49ac-80b6-ecb797fbe613"}}},{"cell_type":"code","source":["fig, ax = plt.subplots(1,1, figsize=(20,5))\n\nax.set_title('Training/validation loss')\n\nplt.plot(history.history['loss'],\n         'b',\n         label='Training loss')\nplt.plot(history.history['val_loss'],\n         'r',\n         label='Validation loss')\nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Loss, [mse]')\nplt.ylim([0,.002])\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84be0338-fc38-4de7-a530-7fe37560ad59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Given the trained model, we are using it to make predictions on the same data it trained and then we calculate the loss in the form of MAE (Mean Average Error) between the predicted values and the actual values.\n\nThe histogram representation of this result (MAE loss) enables us to understand what is the reasonable value for the value that identifies 'normal conditions'. Looking at the right end of the bell shape, we can safely assume that 0.01 is a good value for the threshold.\n\nThis result is remarkable! We have found a limit of the loss function that, if exceeded, means we are moving into anomaly territory. We will use this in the next steps to detect anomalies."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69d99a46-924c-4f60-bcea-c34ea6f8e38d"}}},{"cell_type":"code","source":["X_pred = model.predict(np.array(X_train))\nX_pred = pd.DataFrame(X_pred, \n                      columns=X_train.columns)\nX_pred.index = X_train.index\n\nscored = pd.DataFrame(index=X_train.index)\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n\nfig, ax = plt.subplots(1,1)\nsns.distplot(scored['Loss_mae'],\n             bins = 50, \n             kde= True,\n            color = 'blue');\nplt.xlim([0.0,.02])\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"939a0009-e716-43f5-b5e3-68e77e8bc4ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Save both models (the full autoencoder as well as its half, the encoder) for future use."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43d47a89-49a5-4533-97c4-0a6c2be9a7de"}}},{"cell_type":"code","source":["model.save('anomaly_detection_full_model.h5')\nencoder_model.save('anomaly_detection_encoder_model.h5')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5096a58-5bcb-43b1-a801-22109aa1fe01"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["For here on, we suppose we have a trained model that we use to monitor and detect anomalies in the functioning of the system.\n\nWe are now taking the other two data sets (the one containing gradual failure and the one containing immediate failure) and running them through our full model to get the predicted values. We then compare those predicted values with the input values and calculate for each point in the time series the MAE loss. Finally, we mark each point in the time series as an anomaly or not depending on whether MAE exceeds of not the threshold we determined earlier."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a71a33c-eb15-48fe-8b18-cae218ba675e"}}},{"cell_type":"code","source":["model = load_model('anomaly_detection_full_model.h5')\n\n# Batch scoring for gradual failure\nX_pred1 = model.predict(np.array(X_test1))\nX_pred1 = pd.DataFrame(X_pred1, \n                      columns=X_test1.columns)\nX_pred1.index = X_test1.index\n\nscored1 = pd.DataFrame(index=X_test1.index)\nscored1['Loss_mae'] = np.mean(np.abs(X_pred1-X_test1), axis = 1)\nscored1['Threshold'] = 0.01\nscored1['Anomaly'] = scored1['Loss_mae'] > scored1['Threshold']\n\n# Batch scoring for immediate failure\nX_pred2 = model.predict(np.array(X_test2))\nX_pred2 = pd.DataFrame(X_pred2, \n                      columns=X_test2.columns)\nX_pred2.index = X_test2.index\n\nscored2 = pd.DataFrame(index=X_test2.index)\nscored2['Loss_mae'] = np.mean(np.abs(X_pred2-X_test2), axis = 1)\nscored2['Threshold'] = 0.01\nscored2['Anomaly'] = scored2['Loss_mae'] > scored2['Threshold']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f20bef7-ae7f-494b-9180-e344b649ef72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We are finally ready to plot the results. The blue lines are the actual values of the MAE loss function and the orange line is the threshold we have determined earlier.\n\nNotice how in the case of the gradual failure data set we are detecting the abnormal state quite early, before it reaches the final failed state. In real life, this is extremely important as it allows an automatic shutdown of the system before major failure occurs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90490aa7-6e0a-4041-8604-776e20cd6da2"}}},{"cell_type":"code","source":["fig, ax = plt.subplots(1,2, figsize=(30,5))\n\nax[0].set_title('Failure chart for gradual failure')\nax[0].plot(scored1[['Loss_mae', 'Threshold']])\n\nax[1].set_title('Failure chart for immediate failure')\nax[1].plot(scored2[['Loss_mae', 'Threshold']])\n\nax[0].set_ylabel('Loss (MAE)')\nax[1].set_ylabel('Loss (MAE)')\n\nax[0].legend(('Pump Health Score', 'Failure Threshold'), loc='upper left')\nax[1].legend(('Pump Health Score', 'Failure Threshold'), loc='upper left')\n\nax[0].set_yscale('log')\nax[1].set_yscale('log')\n\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"907dd154-574c-4e18-b5f6-f09ea0cc7d19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now that we have a working model for detecting anomalies in the working of the system, let's try to gain a better understanding of what's happening.\n\nTo do this we will use the encoder portion of our model (the one that 'encodes' each individual state of the 5 sensors into a 20-dimensional vector) and a technique called PCA (Principal Component Analysis).\n\nFirst, we will encode both data sets with anomalies (gradual failure and immediate failure)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0e1d470-f80f-41dc-93c3-7c92b2182a3f"}}},{"cell_type":"code","source":["model = load_model('anomaly_detection_encoder_model.h5')\nX_enc1 = encoder_model.predict(np.array(X_test1))\nX_enc2 = encoder_model.predict(np.array(X_test2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a335ea5-0197-44e2-be1a-98d5fbff709e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We apply several PCA models (with 2, 5, and 10 components respectively) to the state vectors that we obtained in the previous step.\n\nFor the gradual failure dataset, we find out that 2 out of the 20 components of the embedding vectors explain 99.48% of the entire variation found in the data.\nFor the immediate failure dataset, we find out that 2 out of the 20 components of the embedding vectors explain 99.81% of the entire variation found in the data.\n\nAs we move to 5 and 10 components for the PCA, we get some marginal increase in those percentages."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"089edf15-82be-4ea4-b502-3eb52703094c"}}},{"cell_type":"code","source":["pca_components = [2, 5, 10]\ngradual_failure_pca = []\nimmediate_failure_pca = []\n\ndef pca_analysis(input, results, failure_type):\n  for comp in pca_components:\n    pca = PCA(n_components = comp)\n    pca_result = pca.fit_transform(input)\n    print('{} failure - Cumulative explained variation for {} principal components: {}'.format(failure_type, comp, np.sum(pca.explained_variance_ratio_)))\n    results.append(pca_result)\n    \npca_analysis(X_enc1, gradual_failure_pca, 'Gradual')\npca_analysis(X_enc2, immediate_failure_pca, 'Immediate')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2298b9f-8bd1-4406-8b7a-08b93720837e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["To get a visual representation of the 2 components mentioned above, we correlate those 2 components from the embedding vectors with the anomaly flag we've already determined.\n\nWe apply some transformations to improve the readability of the gradual failure dat set."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30219f62-fd37-425e-b54e-f98b57565dc6"}}},{"cell_type":"code","source":["X_embedded1 = pd.DataFrame(gradual_failure_pca[0], columns=['X','Y'])\nX_embedded1['State'] = np.where(scored1['Anomaly'], 'Failure', 'Normal')\nX_embedded1['X_Norm'] = np.log(X_embedded1['X'] * -1 + 20)\nX_embedded1['Y_Norm'] = np.log(X_embedded1['Y'] + 2)\n\nX_embedded2 = pd.DataFrame(immediate_failure_pca[0], columns=['X','Y'])\nX_embedded2['State'] = np.where(scored2['Anomaly'], 'Failure', 'Normal')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83f69b2c-1613-44c1-bef5-b7f79a73520e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we plot the results.\n\nIn each of the two plots, each point represents one individual state of the system (corresponding to one point in time in the time series). The blue points indicate a normal state, and the orange ones indicate a state of anomaly (failure).\n\nIt is quite remarkable how different the two charts are. In the one corresponding to the gradually failing system, most blue points are concentrated into the leftmost part (normal operating conditions). Then we see an arch of orange points that define the gradual failure of the system. The chart corresponding to the immediately failing system, all points are concentrated in two bands - one for normal conditions and one for abnormal conditions. The sudden transition of the system from a normal condition to an abnormal one is very clearly visible."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02ad66a4-5508-4df4-abd2-8aebcca4c64f"}}},{"cell_type":"code","source":["fig, ax = plt.subplots(1, 2, figsize=(30,5))\n\nax[0].set_title('Gradual failure readings')\nax[1].set_title('Immediate failure readings')\n\nsns.scatterplot(x='X_Norm', y='Y_Norm', hue='State', data=X_embedded1, ax=ax[0])\nsns.scatterplot(x='X', y='Y', hue='State', data=X_embedded2, ax=ax[1])\n\ndisplay(fig)\nplt.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3445839f-614c-4fbf-86f1-a4e7a5715985"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Save the model to disk"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eefb562e-a479-4bee-8ebe-2ee38fe11b0e"}}},{"cell_type":"markdown","source":["In preparation for deploying the model, you need to save the model and scaler to a new `models` directory that will be uploaded to the deployment image later on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e6eacf4-f950-4d32-bddb-03f570775233"}}},{"cell_type":"code","source":["import uuid\nimport os\n\n# Create a temporary folder to store locally relevant content for this notebook\ntempFolderName = '/FileStore/mcw_predmaint_{0}'.format(uuid.uuid4())\ndbutils.fs.mkdirs(tempFolderName)\nprint('Content files will be saved to {0}'.format(tempFolderName))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04a7389a-802b-46ab-8686-3c3501950fcb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nmodels_dir = tempFolderName + \"/models/\"\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a297f7de-8caf-4f66-81ab-f8e1b9afd37c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["model = load_model('anomaly_detection_full_model.h5')\nmodel.save(models_dir + 'anomaly_score.h5')\nscaler = joblib.load('scaler.pkl')\njoblib.dump(scaler, models_dir + 'scaler.pkl')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12565d00-1254-4f4f-8e38-4e6c1ec52910"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Test loading the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc4ba986-9602-464d-88da-7fc3803f2c70"}}},{"cell_type":"markdown","source":["All the examples above performed batched predictions. Let's take a look now to an approach for real time scoring. We start with the assumption that we have a previously trained model and scaler.\n\nWe start by loading the models from their persisted state.\n\nNext, we scale the values of one set of sensor readings and run the scaled values through the anomaly detection model.\n\nWe calculate the MAE value (between the predicted values array and the original readings array).\n\nFinally, we compare the MAE value with the threshold to decide whether we have a normal or abnormal condition.\nThe example below detects a normal condition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4b7d501-4d46-493c-9dcb-7e8bd51f21a2"}}},{"cell_type":"code","source":["scaler = joblib.load(models_dir + 'scaler.pkl')\nmodel = load_model(models_dir + 'anomaly_score.h5')\n\nsensor_readings = np.array([70, 200, 60.6, 0, 1448.17])\nscaled_sensor_readings = scaler.transform(sensor_readings.reshape(1,-1))\n\npred_sensor_readings = model.predict(scaled_sensor_readings)\nscore = np.mean(np.abs(scaled_sensor_readings - pred_sensor_readings[0]))\n\nif score > 0.01:\n  print('WARNING! Abnormal conditions detected')\nelse:\n  print('Everything is ok')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdd16b4b-a2c7-439a-a68b-5e56446c6f6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The following example shows how an abnormal condition is detected."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80c918fb-bc05-47d6-875a-99d716b3c372"}}},{"cell_type":"code","source":["sensor_readings = np.array([14.23, 41, 14.4, 318.50, 601.95])\nscaled_sensor_readings = scaler.transform(sensor_readings.reshape(1,-1))\n\npred_sensor_readings = model.predict(scaled_sensor_readings)\nscore = np.mean(np.abs(scaled_sensor_readings - pred_sensor_readings[0]))\n\nif score > 0.01:\n  print('WARNING! Abnormal conditions detected')\nelse:\n  print('Everything is ok')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"794d61fa-09ce-4c12-864f-ca3dbd3801e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Deploy the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3537577d-6541-4aff-9343-db0440c5f94c"}}},{"cell_type":"markdown","source":["Now you will use the Azure Machine Learning service SDK to programmatically register your model and create a container image for the web service that uses it and deploy that image on to an Azure Container Instance.\n\nRun the following cells to create some helper functions that you will use for deployment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46a7aaa3-6028-4866-9193-88f23d52a084"}}},{"cell_type":"code","source":["import azureml\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model as AmlModel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5deb2b3c-f58e-4411-9ed1-caada8859ce3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def getOrCreateWorkspace(subscription_id, resource_group, workspace_name, workspace_region):\n    # By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace instead of an error\n    ws = Workspace.create(\n        name = workspace_name,\n        subscription_id = subscription_id,\n        resource_group = resource_group, \n        location = workspace_region,\n        exist_ok = True)\n    return ws"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad601442-b4fd-4f6c-a5e3-c9eac039ddb6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def deployModelAsWebService(ws, model_paths, model_names, \n                scoring_script_filename=\"score.py\", \n                conda_packages=['scikit-learn','numpy','joblib','pandas','keras','h5py==2.10.0'],\n                conda_file=\"dependencies.yml\", runtime=\"python\",\n                cpu_cores=1, memory_gb=1, tags={'name':'scoring'},\n                description='Scoring web service.',\n                service_name = \"scoringservice\"\n               ):\n\n    print(\"Registering and uploading models...\")\n    registered_models = []\n    \n    for index, path in enumerate(model_paths):\n      \n      print('Registering model {} with name {}'.format(path, model_names[index]))\n    \n      registered_models.append(AmlModel.register(model_path=path, \n                                      model_name=model_names[index], \n                                      workspace=ws))\n    \n    print('Successfully registered {} models.'.format(len(registered_models)))\n\n    # create a Conda dependencies environment file\n    print(\"Creating conda dependencies file locally...\")\n    from azureml.core.conda_dependencies import CondaDependencies \n    mycondaenv = CondaDependencies.create(conda_packages=conda_packages)\n    mycondaenv.add_pip_package('tensorflow==2.2.1')\n    \n    #dbutils.fs.put(tempFolderName + \"/\" + conda_file, mycondaenv.serialize_to_string(), overwrite=True)\n    with open(conda_file,\"w\") as f:\n        f.write(mycondaenv.serialize_to_string())\n    \n    # create inference configuration\n    print(\"Creating inference configuration...\")\n    from azureml.core.model import InferenceConfig\n    inference_config = InferenceConfig(runtime=runtime, \n                                       entry_script=scoring_script_filename,\n                                       conda_file=conda_file)\n    \n    # create ACI configuration\n    print(\"Creating ACI configuration...\")\n    from azureml.core.webservice import AciWebservice, Webservice\n    aci_config = AciWebservice.deploy_configuration(\n        cpu_cores = cpu_cores, \n        memory_gb = memory_gb, \n        tags = tags, \n        description = description)\n\n    # deploy the webservice to ACI\n    print(\"Deploying webservice to ACI...\")\n    webservice = AmlModel.deploy(workspace=ws,\n                              name=service_name,\n                              models=registered_models,\n                              inference_config=inference_config,\n                              deployment_config=aci_config)\n    webservice.wait_for_deployment(show_output=True)\n    \n    return webservice"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d52ae64-a28e-4ad3-9587-d6a7aa12bb44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Your web service which knows how to load the model and use it for scoring needs to be saved out to a file for the Azure Machine Learning service SDK to deploy it. Run the following cell to create this file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0db8357-8a8f-4ebb-9fb0-92ff30b9ab90"}}},{"cell_type":"code","source":["# write out the file scoring-service.py\nscoring_service = \"\"\"\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras\nimport joblib\nfrom tensorflow.keras.models import load_model\nimport h5py\n\nfrom azureml.core.model import Model as AmlModel\n\ndef init():\n  global model\n  global scaler\n  global init_error\n    \n  try:\n    \n    init_error = None\n  \n    scaler_file_path = AmlModel.get_model_path('anomaly-detection-scaler')\n    model_file_path = AmlModel.get_model_path('anomaly-detection-model')\n\n    print('Loading scaler from:', scaler_file_path)\n    scaler = joblib.load(scaler_file_path)\n\n    print('Loading model from:', model_file_path)\n    model = load_model(model_file_path)\n\n  except Exception as e:\n    init_error = e\n    print(e)\n        \n# note you can pass in multiple rows for scoring\ndef run(raw_data):\n\n  if init_error is not None:\n    return 'Init error: {}'.format(str(init_error))\n\n  try:\n    print(\"Received input:\", raw_data)\n    \n    input_df = pd.read_json(raw_data, orient='values')\n    \n    sensor_readings = np.array(input_df)\n    scaled_sensor_readings = scaler.transform(sensor_readings.reshape(1,-1))\n\n    pred_sensor_readings = model.predict(scaled_sensor_readings)\n    score = np.mean(np.abs(scaled_sensor_readings - pred_sensor_readings[0]))\n\n    if score > 0.01:\n      print('WARNING! Abnormal conditions detected')\n      return 1\n    else:\n      print('Everything is ok')\n      return 0\n\n  except Exception as e:\n    error = str(e)\n    return error\n\"\"\"\n\nwith open(tempFolderName + \"/\" + \"score.py\", \"w\") as file:\n    file.write(scoring_service)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d18bf15-4fc2-4e22-964c-927ad523bb51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Next, create your Workspace (or retrieve the existing one if it already exists) and deploy the model.\n\nPlease note that executing the next few cells can take between **7** and **10** minutes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cf9fb8b-47e0-4b4b-b24f-44c6201b6bec"}}},{"cell_type":"markdown","source":["## Configure access to the Azure Machine Learning resources\nTo begin, you will need to provide the following information about your Azure Subscription.\n\n**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n\nIn the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n\nTo get these values, do the following:\n1. Navigate to the Azure Portal and login with your credentials.\n2. From the left-hand menu, under Favorites, select `Resource Groups`.\n3. In the list, select the resource group used for the lab.\n4. Open your Machine Learning Workspace.\n5. From the Overview tab, capture the desired values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c0c518b-1f6e-4205-aeed-237a199c3529"}}},{"cell_type":"code","source":["#Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"\"\n\n#Provide values for the existing Resource Group \nresource_group = \"\"  \n\n#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\nworkspace_name = \"\"\nworkspace_region = \"eastus\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"172ac324-1715-462a-ab89-04f5c527a30a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run the following cells to connect to your **Azure Machine Learning Workspace** If the workspace does not already exist, it will be created. Otherwise the code below will connect to the existing workspace without creating a new one.\n\n**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26a787c7-3354-4362-894d-74021e5fedfb"}}},{"cell_type":"code","source":["ws =  getOrCreateWorkspace(subscription_id, resource_group, \n                   workspace_name, workspace_region)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e81ce3da-ad9e-43a4-bb87-03debbae6b6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# It is important to change the current working directory so that your generated scoring-service.py is at the root of it. \n# This is required by the Azure Machine Learning SDK\nos.chdir(tempFolderName)\nos.getcwd()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2ba7582-f598-4232-a7c4-12f14a36c1d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["webservice = deployModelAsWebService(ws, \n                                     model_paths=['models/anomaly_score.h5', 'models/scaler.pkl'], \n                                     model_names=['anomaly-detection-model', 'anomaly-detection-scaler'])\nprint(webservice.state)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a48c2c1-05d5-4af6-8015-e5f7d2bbc53b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["result = webservice.run('[[14.23, 41, 14.4, 318.50, 601.95]]')\nresult\n# result of 0 = no anomaly detected\n# result of 1 = anomaly detected"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5a03890-10d1-4cf3-a920-b4f3e977a033"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Copy the deployed web service URL\n\nRun the cell below to retrieve the web service URL. Copy the value in the cell's output after running and use it to configure your Azure Function App."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d412646-fde0-454f-93e8-1b62b759d73b"}}},{"cell_type":"code","source":["print(webservice.scoring_uri)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f78e98d5-95e1-4e63-8c89-feb8553c8d63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Anomaly Detection NEWLIBS (2)","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1395029115898365}},"nbformat":4,"nbformat_minor":0}
